#Part 2.calculate the sentiment score at the document level for minutes
#Credit:This is adapted code from https://highdemandskills.com/topic-trends-fomc/
#Note:the names of the variable might be confusing. Because I didn't adjust the names according to its content.
##Import library##
import requests
import re
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import spacy
from pprint import pprint
from pathlib import Path  
import csv
import glob
import re
import string
import sys
import datetime as dt
import MOD_Load_MasterDictionary_v2022 as LM
#this MOD_Load_MasterDictionary_v2022 is downloaded from https://sraf.nd.edu/textual-analysis/code/. It should be put in the same directory as this .py file#

def main():
    print("start main")
    nlp=spacy.load('en_core_web_lg')
    nlp.max_length=1500000

    URLPath=r'https://www.federalreserve.gov/monetarypolicy/fomcminutes'
    URLExt=r'.htm'  
    Minuteslist=[  
        '20090128', '20090318', '20090429', '20090624', '20090812', '20090923', '20091104', '20091216', # 2009 FOMC minutes 
        '20100127', '20100316', '20100428', '20100623', '20100810', '20100921', '20101103', '20101214', # 2010 FOMC minutes 
        '20110126', '20110315', '20110427', '20110622', '20110809', '20110921', '20111102', '20111213', # 2011 FOMC minutes 
        '20120125', '20120313', '20120425', '20120620', '20120801', '20120913', '20121024', '20121212', # 2012 FOMC minutes 
        '20130130', '20130320', '20130501', '20130619', '20130731', '20130918', '20131030', '20131218', # 2013 FOMC minutes 
        '20140129', '20140319', '20140430', '20140618', '20140730', '20140917', '20141029', '20141217', # 2014 FOMC minutes
        '20150128', '20150318', '20150429', '20150617', '20150729', '20150917', '20151028', '20151216', # 2015 FOMC minutes    
        '20160127', '20160316', '20160427', '20160615', '20160727', '20160921', '20161102', '20161214', # 2016 FOMC minutes
        '20170201', '20170315', '20170503', '20170614', '20170726', '20170920', '20171101', '20171213', # 2017 FOMC minutes
        '20180131', '20180321', '20180502', '20180613', '20180801', '20180926', '20181108', '20181219', # 2018 FOMC minutes
        '20190130', '20190320', '20190501', '20190619', '20190731', '20190918', '20191030', '20191211', # 2019 FOMC minutes
        '20200129', '20200315', '20200429', '20200610', '20200729','20200916','20201105','20201216' ,# 2020 FOMC minutes
        '20210127','20210317','20210428','20210616','20210728','20210922','20211103','20211215',#2021 FOMC minutes
        '20220126','20220504','20220316']#2022 FOMC minutes

    
   
  
   
    ##SETTING UP THE CORPUS##
    SentimentCount=[]
    
    # User defined file pointer to LM dictionary
    MASTER_DICTIONARY_FILE = r'C:\Users\Rico\Desktop\Warwick\dissertation\Python\LMgeneral\LoughranMcDonald_MasterDictionary_2021.csv'
    lm_dictionary = LM.load_masterdictionary(MASTER_DICTIONARY_FILE, print_flag=True)
    vdictionary = dict()
    
    #define the function that can generate topics distribution for each sentence
    def format_topics_sentences(datetemp,sentimentCount):
    # Init output
        sent_topics_df = pd.DataFrame()
        score_negative=pd.Series()
        score_positive=pd.Series()
        score_uncertainty=pd.Series()
        a=0
        for i in datetemp:

            Odata=sentimentCount[a]
            if Odata[0]!=0:
                score_negative=Odata[1]/Odata[0]
                score_positive=Odata[2]/Odata[0]
                score_uncertainty=Odata[3]/Odata[0]
            else:
                score_negative=-20
                score_positive=-20
                score_uncertainty=-20


            sent_topics_df = sent_topics_df.append(pd.Series([score_negative,score_positive,score_uncertainty]), ignore_index=True)
            a=a+1
                    
        DATETEMP=pd.Series(datetemp)
        sent_topics_df = pd.concat([DATETEMP,sent_topics_df], axis=1)
           
            

        return(sent_topics_df)


    #Define function to prepare corpus
    print("start prepare Corpus")
    def Preparetext(urlpath,urlext,minslist1,minparalength):
        date=[]
        sentimentcount=[]
    
        

        for minutes in minslist1:

            response=requests.get(urlpath+minutes+urlext)#get the URL response
            soup=BeautifulSoup(response.content,'lxml')#parse the response
            #extract minutes content and convert to string
            minsTxt=str(soup.find("div",{"id":"content"}))#containted within the 'div' tag

            #Clean text- stage1
            minsTxt=minsTxt.strip() #remove white spcace at the beginning and end
            minsTxt=minsTxt.replace('\r','')#replace the \r with null
            minsTxt=minsTxt.replace(' ',' ')#replace space with ' '
            while '  ' in minsTxt:
                minsTxt=minsTxt.replace('  ',' ')#remove extra spaces
            #Clean text-stage 2,using regex(as SpaCy incorrectly parses certain HTML tags)
            minsTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags
                        '([_]+)|' # Remove series of underscores
                        '(http[^\s]+)|' # Remove website addresses
                        'a.'
                        'i.'
                        '((a|p)\.m\.)', # Remove "a.m" and "p.m."
                        '', minsTxt) # Replace with null
            
            #Find length of minutes document for calculating paragraph weights 
            _odata=[0]*8
            date.append(minutes)
            #Parse cleaned para with SpaCy
            minsPara=nlp(minsTxt)
            #Further cleaning and selection of text characteristics
            # Further cleaning and selection of text characteristics
            for token in minsPara:
                if token.is_stop == False and token.is_punct == False and (token.pos_ == "NOUN" or token.pos_ == "ADJ" or token.pos_ =="VERB"): # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb
                    #sentiment count
                    try:
                        if token.lemma_.upper() in lm_dictionary:
                            _odata[0] += 1  # word count 
                            if token.lemma_.upper() not in vdictionary:vdictionary[token] = 1
                            if lm_dictionary[token.lemma_.upper()].negative: _odata[1] += 1
                            if lm_dictionary[token.lemma_.upper()].positive: _odata[2] += 1
                            if lm_dictionary[token.lemma_.upper()].uncertainty: _odata[3] += 1
                            if lm_dictionary[token.lemma_.upper()].litigious: _odata[4] += 1
                            if lm_dictionary[token.lemma_.upper()].strong_modal: _odata[5] += 1
                            if lm_dictionary[token.lemma_.upper()].weak_modal: _odata[6] += 1
                            if lm_dictionary[token.lemma_.upper()].constraining: _odata[7] += 1
                    except KeyError:
                        pass
            sentimentcount.append(_odata)

        return date,sentimentcount
    

    DATE,SentimentCount= Preparetext(urlpath=URLPath,urlext=URLExt, minslist1=Minuteslist, minparalength=200)
    print(SentimentCount)
    df_topic_sents_keywords = format_topics_sentences(datetemp=DATE,sentimentCount=SentimentCount)


    # Format
    filepath = Path(r'C:\Users\Rico\Desktop\Warwick\dissertation\Python\wholescoresminutes.csv')  
    filepath.parent.mkdir(parents=True, exist_ok=True)  
    df_topic_sents_keywords.to_csv(filepath)
if __name__ == '__main__':
    main()
