##Import library##
import requests
import re
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import spacy
from pprint import pprint
from pathlib import Path  
import csv
import glob
import re
import string
import sys
import datetime as dt
import MOD_Load_MasterDictionary_v2022 as LM
#this MOD_Load_MasterDictionary_v2022 is downloaded from https://sraf.nd.edu/textual-analysis/code/. It should be put in the same directory as this .py file#



def main():
    print("start main")
    nlp=spacy.load('en_core_web_lg')
    nlp.max_length=1500000

    ##grab the documents by parsing URLs##
    #define URLs for the specific FOMC minutes
    URLPath=r'https://www.federalreserve.gov/newsevents/pressreleases/monetary'
    URLExt=r'a.htm'
    #list for FOMC statement from 2009 onward
    Statementlist=[
            '20090128', '20090318', '20090429', '20090624', '20090812', '20090923', '20091104', '20091216', # 2009 FOMC statement 
            '20100127', '20100316', '20100428', '20100623', '20100810', '20100921', '20101103', '20101214', # 2010 FOMC statement 
            '20110126', '20110315', '20110427', '20110622', '20110809', '20110921', '20111102', '20111213', # 2011 FOMC statement 
            '20120125', '20120313', '20120425', '20120620', '20120801', '20120913', '20121024', '20121212', # 2012 FOMC statement
            '20130130', '20130320', '20130501', '20130619', '20130731', '20130918', '20131030', '20131218', # 2013 FOMC statement
            '20140129', '20140319', '20140430', '20140618', '20140730', '20140917', '20141029', '20141217', # 2014 FOMC statement
            '20150128', '20150318', '20150429', '20150617', '20150729', '20150917', '20151028', '20151216', # 2015 FOMC statement    
            '20160127', '20160316', '20160427', '20160615', '20160727', '20160921', '20161102', '20161214', # 2016 FOMC statement
            '20170201', '20170315', '20170503', '20170614', '20170726', '20170920', '20171101', '20171213', # 2017 FOMC statement
            '20180131', '20180321', '20180502', '20180613', '20180801', '20180926', '20181108', '20181219', # 2018 FOMC statement
            '20190130', '20190320', '20190501', '20190619', '20190731', '20190918', '20191030', '20191211', # 2019 FOMC statement
            '20200129', '20200315', '20200429', '20200610', '20200729','20200916','20201105','20201216' ,# 2020 FOMC statement
            '20210127','20210317','20210428','20210616','20210728','20210922','20211103','20211215',#2021 FOMC statement
            '20220126','20220316','20220504']#2022 FOMC statement
    
    
    # User defined file pointer to LM dictionary
    MASTER_DICTIONARY_FILE = r'C:\Users\Rico\Desktop\Warwick\dissertation\Python\LMgeneral\LoughranMcDonald_MasterDictionary_2021.csv'
    lm_dictionary = LM.load_masterdictionary(MASTER_DICTIONARY_FILE, print_flag=True)
    vdictionary = dict()
    SentimentCount=[]
    
    #define the function that can generate topics distribution for each sentence
    def format_topics_sentences(datetemp,sentimentCount):
    # Init output
        sent_topics_df = pd.DataFrame()
        score_negative=pd.Series()
        score_positive=pd.Series()
        score_uncertainty=pd.Series()
        a=0
        for i in datetemp:

            Odata=sentimentCount[a]
            if Odata[0]!=0:
                score_negative=Odata[1]/Odata[0]
                score_positive=Odata[2]/Odata[0]
                score_uncertainty=Odata[3]/Odata[0]
            else:
                score_negative=-20
                score_positive=-20
                score_uncertainty=-20


            sent_topics_df = sent_topics_df.append(pd.Series([score_negative,score_positive,score_uncertainty]), ignore_index=True)
            a=a+1
                    
        DATETEMP=pd.Series(datetemp)
        sent_topics_df = pd.concat([DATETEMP,sent_topics_df], axis=1)
           
            

        return(sent_topics_df)


    #Define function to prepare text
    print("start prepare Corpus")
    def Preparetext(urlpath,urlext,stalist):
        fomcsta=[]
        date=[]
        sentimentcount=[]
    
        

        for statements in stalist:

            response=requests.get(urlpath+statements+urlext)#get the URL response
            soup=BeautifulSoup(response.content,'lxml')#parse the response
            #extract minutes content and convert to string
            staTxt=str(soup.find("div",{"id":"article"}))#containted within the 'div' tag

            #Clean text- stage1
            staTxt=staTxt.strip() #remove white spcace at the beginning and end
            staTxt=staTxt.replace('\r','')#replace the \r with null
            staTxt=staTxt.replace(' ',' ')
            while '  ' in staTxt:
                staTxt=staTxt.replace('  ',' ')#remove extra spaces
            #Clean text-stage 2,using regex(as SpaCy incorrectly parses certain HTML tags)
            staTxt = re.sub(r'(<[^>]*>)|' # Remove content within HTML tags
                        '([_]+)|' # Remove series of underscores
                        '(http[^\s]+)|' # Remove website addresses
                        'a.'
                        'i.'
                        '((a|p)\.m\.)', # Remove "a.m" and "p.m."
                        '', staTxt) # Replace with null
            
            #Find length of minutes document for calculating paragraph weights 
            _odata=[0]*8
            date.append(statements)
            #Parse cleaned para with SpaCy
            staPara=nlp(staTxt)
            # Further cleaning and selection of text characteristics
            for token in staPara:
                if token.is_stop == False and token.is_punct == False and (token.pos_ == "NOUN" or token.pos_ == "ADJ" or token.pos_ =="VERB"): # Retain words that are not a stop word nor punctuation, and only if a Noun, Adjective or Verb
                    #sentiment count
                    try:
                        if token.lemma_.upper() in lm_dictionary:
                            _odata[0] += 1  # word count 
                            if token.lemma_.upper() not in vdictionary:vdictionary[token] = 1
                            if lm_dictionary[token.lemma_.upper()].negative: _odata[1] += 1
                            if lm_dictionary[token.lemma_.upper()].positive: _odata[2] += 1
                            if lm_dictionary[token.lemma_.upper()].uncertainty: _odata[3] += 1
                            if lm_dictionary[token.lemma_.upper()].litigious: _odata[4] += 1
                            if lm_dictionary[token.lemma_.upper()].strong_modal: _odata[5] += 1
                            if lm_dictionary[token.lemma_.upper()].weak_modal: _odata[6] += 1
                            if lm_dictionary[token.lemma_.upper()].constraining: _odata[7] += 1
                    except KeyError:
                        pass
            sentimentcount.append(_odata)

        return date,sentimentcount
    

    DATE,SentimentCount= Preparetext(urlpath=URLPath,urlext=URLExt, stalist=Statementlist)
    print(SentimentCount)
    df_topic_sents_keywords = format_topics_sentences(datetemp=DATE,sentimentCount=SentimentCount)


    # save the result
    filepath = Path(r'C:\Users\Rico\Desktop\Warwick\dissertation\Python\wholescorestatement.csv')  
    filepath.parent.mkdir(parents=True, exist_ok=True)  
    df_topic_sents_keywords.to_csv(filepath)
if __name__ == '__main__':
    main()
